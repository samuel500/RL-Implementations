{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7sWp4sRM66z"
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylWlA14XNT35"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from baselines.common.atari_wrappers import wrap_deepmind\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import statistics\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from IPython import display\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow import summary\n",
    "#%load_ext tensorboard\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzBqpwnHNeKI"
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    \n",
    "    def __init__(self, size=10000):\n",
    "        self.size=size\n",
    "        self.cursor = 0\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, exp):\n",
    "        # exp = deepcopy(exp)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer[self.cursor] = exp\n",
    "        self.cursor += 1\n",
    "        if self.cursor == self.size:\n",
    "            self.cursor = 0\n",
    "            \n",
    "        \n",
    "    \n",
    "    def sample(self, sample_size):\n",
    "        return np.array(random.sample(self.buffer, k=sample_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxqAjTMjjGjP"
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[strides], padding='VALID') # [filter_height, filter_width, in_channels, out_channels]\n",
    "    x = x+b #tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usSqyKGdRflR"
   },
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "\n",
    "    def __init__(self, num_actions, obs_dim, scope, atari):\n",
    "        \n",
    "        self.scope = scope\n",
    "        self.out_dim = num_actions\n",
    "        self.obs_dim = obs_dim\n",
    "        self.atari = atari\n",
    "        with tf.variable_scope(scope):\n",
    "\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [None] + list(obs_dim))\n",
    "            self.Y = tf.placeholder(tf.float32)\n",
    "            self.M = tf.placeholder(tf.float32, [None, self.out_dim])\n",
    "            fc_nb = 256\n",
    "            if atari:\n",
    "                self.layers = [\n",
    "                    {'type': 'conv', 'in': self.obs_dim[-1], 'out': 16, 'height': 8, 'width': 8, 'stride': 4},\n",
    "                    {'type': 'conv', 'in': 16, 'out': 32, 'height': 4, 'width': 4, 'stride': 2},\n",
    "                    {'type': 'fc', 'n': fc_nb, 'prev': 9*9*32},\n",
    "                    {'type': 'fc', 'n': self.out_dim, 'prev': fc_nb}\n",
    "                ]\n",
    "                \"\"\"\n",
    "                self.layers = [\n",
    "                    {'type': 'conv', 'in': self.obs_dim[-1], 'out': 32, 'height': 8, 'width': 8, 'stride': 4},\n",
    "                    {'type': 'conv', 'in': 32, 'out': 64, 'height': 4, 'width': 4, 'stride': 2},\n",
    "                    {'type': 'conv', 'in': 64, 'out': 64, 'height': 3, 'width': 3, 'stride': 1},\n",
    "                    {'type': 'fc', 'n': fc_nb, 'prev': 7*7*64},\n",
    "                    {'type': 'fc', 'n': self.out_dim, 'prev': fc_nb}\n",
    "                ]\n",
    "                \"\"\"\n",
    "\n",
    "            else:\n",
    "                self.layers = [\n",
    "                    {'type': 'fc', 'n': 64, 'prev': self.obs_dim[-1]},\n",
    "                    {'type': 'fc', 'n': 64, 'prev': 64},\n",
    "\n",
    "                    {'type': 'fc', 'n': self.out_dim, 'prev': 64}\n",
    "                ]\n",
    "\n",
    "            self.wb = []\n",
    "            initializer = tf.initializers.variance_scaling(scale=2.0)\n",
    "            #initializer = tf.initializers.variance_scaling()\n",
    "\n",
    "            #initializer = tf.contrib.layers.xavier_initializer()\n",
    "            #initializer = tf.random_normal_initializer()\n",
    "            for layer in self.layers:\n",
    "                if layer['type'] == 'conv':\n",
    "                    self.wb.append([tf.Variable(initializer([layer['height'], layer['width'], layer['in'], layer['out']])),\n",
    "                                    tf.Variable(tf.zeros([layer['out']]))])\n",
    "                elif layer['type'] == 'fc':\n",
    "                    self.wb.append([tf.Variable(initializer([layer['prev'], layer['n']])),\n",
    "                                    tf.Variable(tf.zeros([layer['n']]))])\n",
    "\n",
    "            self.forward = self.graph(self.X, self.M)\n",
    "        \n",
    "        \n",
    "    def graph(self, x, mask):\n",
    "        if self.atari:\n",
    "            x = x/255.0\n",
    "        for i, wb in enumerate(self.wb):\n",
    "            if self.layers[i]['type'] == 'conv':\n",
    "                x = conv2d(x, wb[0], wb[1], strides=self.layers[i]['stride'])\n",
    "            else:\n",
    "                x = tf.contrib.layers.flatten(x)\n",
    "                x = tf.matmul(x, wb[0])+wb[1]\n",
    "                if i+1<len(self.wb):\n",
    "                    x = tf.nn.relu(x)   \n",
    "        x = x*mask\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def copy_model_parameters(self, sess, copy_scope = 'main'):\n",
    "        \"\"\"\n",
    "        Copies the model parameters of one estimator to another.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session instance\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(copy_scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        #print('e1_params', e1_params)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(self.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "        #print('e2_params', e2_params)\n",
    "\n",
    "        update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            update_ops.append(op)\n",
    "\n",
    "        sess.run(update_ops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvf9mgsKUvYW"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, env, sess, lr=1e-4, gamma=0.99, buffer_size=100000,\n",
    "                 device=None, epoch_steps=5e4, evaluation_runs=5, batch_size=32, \n",
    "                 multiplier=1, target_network_update_freq = 2000, epsilon_decay_steps = 5e5,\n",
    "                 start_buffer=20000, atari=True):\n",
    "        \n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "        self.atari = atari\n",
    "        self.epoch = 0\n",
    "    \n",
    "        self.exp_buf = ExperienceBuffer(buffer_size)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.multiplier = multiplier\n",
    "        self.batch_size = batch_size*multiplier\n",
    "        \n",
    "        self.evaluation_runs = evaluation_runs\n",
    "        self.epoch_steps = epoch_steps\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.shape\n",
    "        self.epsilon_start = 1\n",
    "        self.epsilon_end = 0.1\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        \n",
    "        \n",
    "        self.qnet = QNetwork(self.num_actions, self.obs_dim, atari=atari, scope='main')\n",
    "        \n",
    "        \n",
    "        self.target_network = QNetwork(self.num_actions, self.obs_dim, atari=atari, scope='target')\n",
    "        \n",
    "        self.target_network_update_freq = target_network_update_freq\n",
    "        \n",
    "        #self.qnet_loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions = self.qnet.forward, labels=self.qnet.Y))\n",
    "        self.qnet_loss = tf.reduce_mean(tf.losses.huber_loss(predictions = self.qnet.forward, labels=self.qnet.Y, delta=5.0))\n",
    "        # self.loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "        \n",
    "        self.tf_loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "\n",
    "        self.loss_summary = tf.summary.scalar('loss', self.tf_loss_ph)\n",
    "        #self.summary = tf.summary.merge_all()\n",
    "        self.writer = tf.summary.FileWriter('logs/', self.sess.graph)\n",
    "\n",
    "\n",
    "        self.qnet_optimizer = tf.train.AdamOptimizer(lr).minimize(self.qnet_loss)\n",
    "        #self.qnet_optimizer = tf.train.RMSPropOptimizer(lr, momentum=0.95).minimize(self.qnet_loss)\n",
    "        #self.qnet_optimizer = tf.train.RMSPropOptimizer(lr).minimize(self.qnet_loss)\n",
    "            \n",
    "\n",
    "        self.steps = 0\n",
    "        self.best_eval = -99999\n",
    "        \n",
    "        self.initialize_buffer(start_buffer)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    def save_model(self):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, './Epoch' + str(self.epoch) + '-Pong.ckpt')\n",
    "        print('saved')\n",
    "        \n",
    "    def initialize_buffer(self, steps=20000):\n",
    "        done = True\n",
    "        obs = None\n",
    "        print(\"\\nInitializing buffer:\")\n",
    "        for _ in tqdm(range(steps)):\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "            init_obs = obs\n",
    "            act = self.env.action_space.sample()\n",
    "            obs, rew, done, _ = self.env.step(act)\n",
    "            self.exp_buf.add([init_obs, act, rew, obs, not done])\n",
    "\n",
    "            \n",
    "    def choose_action(self, obs):\n",
    "        if random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            nn_input = {self.qnet.X: [obs], self.qnet.M: np.array([self.num_actions*[1]])}\n",
    "            action = self.qnet.forward.eval(nn_input, session=self.sess)\n",
    "            action = np.argmax(action[0])\n",
    "            return action\n",
    "        \n",
    "\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \n",
    "        self.epoch += 1\n",
    "        i = 0\n",
    "        print(\"====Epoch:\", self.epoch, \"====\")\n",
    "        print('steps', self.steps)\n",
    "        print('epsilon', self.epsilon)\n",
    "        \n",
    "        losss = []\n",
    "\n",
    "        rew_list = []\n",
    "        diff_list = []\n",
    "        with tqdm(total=self.epoch_steps) as pbar:\n",
    "\n",
    "            while i < self.epoch_steps:\n",
    "                \n",
    "                obs = self.env.reset()\n",
    "\n",
    "                done = False\n",
    "                tot_rew = 0\n",
    "                \n",
    "\n",
    "                while (not done) and (i < self.epoch_steps):\n",
    "                    \n",
    "                    step_num = i + (self.epoch-1)*self.epoch_steps\n",
    "                    \n",
    "                    for e in range(self.multiplier):\n",
    "                        init_obs = obs\n",
    "                        act = self.choose_action(obs)\n",
    "                        obs, rew, done, _ = self.env.step(act)\n",
    "\n",
    "                        tot_rew += rew\n",
    "                        self.exp_buf.add([init_obs, act, rew, obs, not done])\n",
    "\n",
    "                        if done:\n",
    "                            rew_list.append(tot_rew)\n",
    "                            break\n",
    "\n",
    "                    if not (self.steps)%self.target_network_update_freq:\n",
    "                        self.target_network.copy_model_parameters(self.sess, self.qnet.scope)\n",
    "                    \n",
    "                    # training \n",
    "                    st = time()\n",
    "\n",
    "                    sample = self.exp_buf.sample(self.batch_size)\n",
    "                    \n",
    "                    #assert all(s.shape == (84, 84, 4) for s in sample[:,3])\n",
    "                    DDQN = True\n",
    "                    verb = False\n",
    "\n",
    "                    if DDQN:\n",
    "                        nn_input = {self.qnet.X: np.stack(sample[:,3]), self.qnet.M: np.ones((len(sample), self.num_actions))}\n",
    "                        nextqs = self.qnet.forward.eval(nn_input, session=self.sess)\n",
    "                        #target_q = np.amax(nextqs, 1)\n",
    "\n",
    "                        maxq_index = np.argmax(nextqs, axis=1)\n",
    "                        #if not self.steps%10000:\n",
    "                        if verb and not self.steps%1:\n",
    "                            print('nextqs', nextqs)\n",
    "                            print('maxq_index', maxq_index)\n",
    "                    \n",
    "                    nn_input = {self.target_network.X: np.stack(sample[:,3]), self.target_network.M: np.ones((len(sample), self.num_actions))}\n",
    "                    nextqs = self.target_network.forward.eval(nn_input, session=self.sess)\n",
    "                    if verb and not self.steps%1:\n",
    "                        print('nextqs_target', nextqs)\n",
    "                    \n",
    "                    if DDQN:\n",
    "                        target_q = nextqs[range(len(nextqs)), maxq_index]\n",
    "                        #print()\n",
    "                        #if not self.steps%10000:\n",
    "                        if verb and not self.steps%1:\n",
    "                            print('target_q', target_q)\n",
    "                            print('DQN target_q', np.amax(nextqs, 1))\n",
    "                        #if not self.steps%1:\n",
    "                        #    diff_list.append(sum((target_q-np.amax(nextqs, 1))**2 > 0))\n",
    "                            #print(\"diff\", sum((target_q-np.amax(nextqs, 1))**2 > 0))\n",
    "\n",
    "                    else:\n",
    "                        target_q = np.amax(nextqs, 1)\n",
    "                    \n",
    "\n",
    "                    \n",
    "\n",
    "                    ys = sample[:,2] + sample[:,4] * self.gamma * target_q\n",
    "                    \n",
    "                    #if not self.steps%1:\n",
    "                    #    print('ys', ys)\n",
    "                    #    print('dqn ys', sample[:,2] + sample[:,4] * self.gamma * np.amax(nextqs, 1))\n",
    "                    #    print(\"diff2\", sum((ys-(sample[:,2] + sample[:,4] * self.gamma * np.amax(nextqs, 1)))**2))\n",
    "                    \n",
    "                    \n",
    "                    action_mask = np.eye(self.num_actions)[sample[:,1].astype(int)]\n",
    "                    if verb:\n",
    "                        print('actions', sample[:,1])\n",
    "                        print('action mask', action_mask)\n",
    "                    #ys_dqn = np.multiply(sample[:,2] + sample[:,4] * self.gamma * np.amax(nextqs, 1), action_mask.T).T\n",
    "                    ys = np.multiply(ys, action_mask.T).T\n",
    "                    #print('ys', ys)\n",
    "                    if verb and not self.steps%10000:\n",
    "                        print('ys', ys)\n",
    "                    #print('ys_dqn', ys_dqn, '\\n')\n",
    "                    \n",
    "                    if not self.steps%150000 and self.steps:\n",
    "                        print('ys', ys[:8])\n",
    "                        \n",
    "                    \n",
    "                    #print(\"b:\", time()-st)\n",
    "\n",
    "                    st = time()\n",
    "                    \n",
    "                    #if not self.steps%10:\n",
    "                    summ, c = self.sess.run([self.qnet_optimizer, self.qnet_loss], feed_dict = {self.qnet.X: np.stack(sample[:,0]), self.qnet.M: action_mask, self.qnet.Y: ys})\n",
    "                    loss_summ = self.sess.run(self.loss_summary, feed_dict = {self.tf_loss_ph: c})\n",
    "                    self.writer.add_summary(loss_summ, self.steps)\n",
    "                    #else:\n",
    "                    #    summ, c = self.sess.run([self.qnet_optimizer, self.qnet_loss], feed_dict = {self.qnet.X: np.stack(sample[:,0]), self.qnet.M: action_mask, self.qnet.Y: ys})\n",
    "\n",
    "\n",
    "                    losss.append(c)\n",
    "                    \n",
    "                    \n",
    "                    self.steps += 1\n",
    "                    i += 1\n",
    "                    pbar.update(1)\n",
    "                    #print(\"c:\", time()-st)\n",
    "        print(\"\\nAvg rew:\", statistics.mean(rew_list))\n",
    "        if self.best_eval < statistics.mean(rew_list):\n",
    "            self.best_eval = statistics.mean(rew_list)\n",
    "        print(\"losss:\", statistics.mean(list(map(float, losss))))\n",
    "        #print(diff_list)\n",
    "        self.run_evaluation()\n",
    "        \n",
    "        #self.save_model()\n",
    "\n",
    "    \n",
    "    def run_evaluation(self):\n",
    "        done = False\n",
    "        obs = None\n",
    "        \n",
    "        rs = []\n",
    "        for r in tqdm(range(self.evaluation_runs)):\n",
    "            tot_rew = 0\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            noopAct = random.randint(0,10)\n",
    "            for i in range(50000):\n",
    "                #render = True\n",
    "                \n",
    "                #if render:\n",
    "                #    self.env.render()\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "                if i < noopAct: # https://arxiv.org/pdf/1511.06581.pdf\n",
    "                    act = 0 # env.action_space.sample()\n",
    "                #if random.random() < 0.01:\n",
    "                \n",
    "                #act = env.action_space.sample()\n",
    "                else:\n",
    "                    nn_input = {self.qnet.X: [obs], self.qnet.M: np.ones((1, self.num_actions))}\n",
    "                    action = self.qnet.forward.eval(nn_input, session=self.sess)\n",
    "                    act = np.argmax(action[0])\n",
    "                obs, rew, done, _ = self.env.step(act)\n",
    "                tot_rew += rew\n",
    "            rs.append(tot_rew)\n",
    "        \n",
    "        print(\"test rewards:\", rs)\n",
    "        \n",
    "        \n",
    "    def display_agent(self):\n",
    "        import io\n",
    "        import base64\n",
    "        from IPython.display import HTML\n",
    "\n",
    "        env = gym.wrappers.Monitor(self.env, \"./gym-results\", force=True)\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        for _ in range(50000):\n",
    "            if done:\n",
    "                break\n",
    "            nn_input = {self.qnet.X: [obs], self.qnet.M: np.ones((1, self.num_actions))}\n",
    "            action = self.qnet.forward.eval(nn_input, session=self.sess)\n",
    "                        #print(action)\n",
    "            act = np.argmax(action[0])\n",
    "            #time.sleep(0.003)\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "            #env.render()\n",
    "        env.close()\n",
    "\n",
    "\n",
    "\n",
    "        video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        HTML(data='''\n",
    "            <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    "        .format(encoded.decode('ascii')))\n",
    "    \n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        eps = self.epsilon_start - (self.epsilon_start-self.epsilon_end) * self.steps / self.epsilon_decay_steps\n",
    "        eps = max(self.epsilon_end, eps)\n",
    "        return eps\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LNqUxigDRiCv",
    "outputId": "5186e3b6-2ad7-4fb6-fe95-7763548d0148"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 7114/20000 [00:00<00:00, 71133.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing buffer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 42226.04it/s]\n",
      "  0%|          | 0/10000.0 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "====Epoch: 1 ====\n",
      "steps 0\n",
      "epsilon 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000.0 [00:52<00:00, 189.65it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg rew: 25.713917525773194\n",
      "losss: 0.02063869594070711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 11.69it/s]\n",
      "  0%|          | 1/10000.0 [00:00<17:23,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rewards: [10.0, 9.0, 172.0, 221.0, 10.0]\n",
      "1\n",
      "====Epoch: 2 ====\n",
      "steps 10000\n",
      "epsilon 0.8200000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000.0 [00:51<00:00, 195.18it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg rew: 31.50473186119874\n",
      "losss: 0.046496653694927226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.96it/s]\n",
      "  0%|          | 1/10000.0 [00:00<26:46,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rewards: [9.0, 9.0, 199.0, 279.0, 9.0]\n",
      "2\n",
      "====Epoch: 3 ====\n",
      "steps 20000\n",
      "epsilon 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/10000.0 [00:00<03:03, 54.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-89e580c82964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4c96882ee5d9>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;31m#assert all(s.shape == (84, 84, 4) for s in sample[:,3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3795062ce364>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mselected_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#if __name__ == '__main__':\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "#env = gym.make('Pong-v4')\n",
    "#env = gym.make('PongNoFrameskip-v4')\n",
    "#env = gym.make('PongDeterministic-v4')\n",
    "#env = gym.make('Breakout-v0')\n",
    "#env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "#sess = tf.Session()\n",
    "\n",
    "atari = False\n",
    "if 'atari' in str(env.env).lower():\n",
    "    print('atari')\n",
    "    env = wrap_deepmind(env, frame_stack=True, episode_life=False)\n",
    "    atari = True   \n",
    "    dqn_args = {\n",
    "        \"env\": env,\n",
    "        \"sess\": sess,\n",
    "        \"multiplier\": 1,\n",
    "        'lr':2e-4,\n",
    "        'buffer_size':100000,\n",
    "        'epoch_steps':5e4,\n",
    "        'gamma': 0.99,\n",
    "        'target_network_update_freq': 5000,\n",
    "        'epsilon_decay_steps': 6e5,\n",
    "        'start_buffer': 40000,\n",
    "        'batch_size': 32,\n",
    "        'atari': atari\n",
    "\n",
    "    }\n",
    "\n",
    "else:\n",
    "    dqn_args = {\n",
    "    'env': env,\n",
    "    'sess': sess,\n",
    "    'lr': 1e-3,\n",
    "    'buffer_size':50000,\n",
    "    'epoch_steps':1e4,\n",
    "    'gamma': 0.95,\n",
    "    'target_network_update_freq': 1000,\n",
    "    'epsilon_decay_steps': 5e4,\n",
    "    'atari': atari,\n",
    "\n",
    "    }\n",
    "    \n",
    "#allow_soft_placement=True,\n",
    "\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "dqn = DQN(**dqn_args)\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(e)\n",
    "    dqn.train_epoch()\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"    \n",
    "    steps = 200\n",
    "    done = True\n",
    "    obs = None\n",
    "    #for i in tqdm(range(steps)):\n",
    "    for i in range(steps):\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        #self.exp_buf.store(init_obs, act, rew, obs, done)\n",
    "        #plt.imshow(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())    \n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        \n",
    "        init_obs = obs\n",
    "        act = env.action_space.sample()\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "        \n",
    "    env.close()\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKRqESHIfweC"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import time\n",
    "\n",
    "#env = gym.make('Pong-v0')\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "env = wrap_deepmind(env, frame_stack=True, episode_life=False)\n",
    "\n",
    "\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "for _ in range(50000):\n",
    "    if done:\n",
    "        break\n",
    "    act = env.action_space.sample()\n",
    "    #time.sleep(0.003)\n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    #env.render()\n",
    "env.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlijKW1JtyVI"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-GiWL8lUz4W"
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "\n",
    "\n",
    "#%tensorboard --logdir logs\n",
    "#print(dqn.epoch)\n",
    "#dqn.run_evaluation()\n",
    "print(dqn.best_eval)\n",
    "\n",
    "dqn.run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eW9Igq9yfkAo"
   },
   "outputs": [],
   "source": [
    "dqn.display_agent()\n",
    "    #!pip uninstall tensorflow-gpu\n",
    "  #!pip install tensorflow-gpu"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "double DQN_tensor 21.08.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
