{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7sWp4sRM66z"
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylWlA14XNT35"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from atari_wrappers import wrap_deepmind\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import statistics\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from IPython import display\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow import summary\n",
    "#%load_ext tensorboard\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzBqpwnHNeKI"
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    \n",
    "    def __init__(self, size=10000):\n",
    "        self.size=size\n",
    "        self.cursor = 0\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, exp):\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(exp)\n",
    "        else:\n",
    "            self.buffer[self.cursor] = exp\n",
    "        self.cursor += 1\n",
    "        if self.cursor == self.size:\n",
    "            self.cursor = 0   \n",
    "    \n",
    "    def sample(self, sample_size):\n",
    "        return np.array(random.sample(self.buffer, k=sample_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxqAjTMjjGjP"
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "\n",
    "    x = tf.nn.conv2d(x, W, strides=[strides], padding='VALID') # [filter_height, filter_width, in_channels, out_channels]\n",
    "    x = x+b #tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usSqyKGdRflR"
   },
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "\n",
    "    def __init__(self, num_actions, obs_dim, scope, atari, dueling=True):\n",
    "        \n",
    "        self.scope = scope\n",
    "        self.out_dim = num_actions\n",
    "        self.obs_dim = obs_dim\n",
    "        self.atari = atari\n",
    "        self.dueling = dueling\n",
    "        with tf.variable_scope(scope):\n",
    "\n",
    "            self.X = tf.placeholder(tf.float32, [None] + list(obs_dim))\n",
    "            self.Y = tf.placeholder(tf.float32)\n",
    "            self.M = tf.placeholder(tf.float32, [None, self.out_dim])\n",
    "            fc_nb = 512\n",
    "            if atari:\n",
    "                if dueling:\n",
    "                    self.layers = [\n",
    "                        {'type': 'conv', 'in': self.obs_dim[-1], 'out': 32, 'height': 8, 'width': 8, 'stride': 4},\n",
    "                        {'type': 'conv', 'in': 32, 'out': 64, 'height': 4, 'width': 4, 'stride': 2},\n",
    "                        {'type': 'conv', 'in': 64, 'out': 64, 'height': 3, 'width': 3, 'stride': 1},\n",
    "                        [{'type': 'fc', 'n': fc_nb, 'prev': 7*7*64}, {'type': 'fc', 'n': fc_nb, 'prev': 7*7*64}],\n",
    "                        [{'type': 'fc', 'n': 1, 'prev': fc_nb}, {'type': 'fc', 'n': self.out_dim, 'prev': fc_nb}] \n",
    "                    ]\n",
    "                else:\n",
    "                    self.layers = [\n",
    "                        {'type': 'conv', 'in': self.obs_dim[-1], 'out': 16, 'height': 8, 'width': 8, 'stride': 4},\n",
    "                        {'type': 'conv', 'in': 16, 'out': 32, 'height': 4, 'width': 4, 'stride': 2},\n",
    "                        {'type': 'fc', 'n': fc_nb, 'prev': 9*9*32},\n",
    "                        {'type': 'fc', 'n': self.out_dim, 'prev': fc_nb}\n",
    "                    ]\n",
    "            else:\n",
    "                self.layers = [\n",
    "                    {'type': 'fc', 'n': 64, 'prev': self.obs_dim[-1]},\n",
    "                    {'type': 'fc', 'n': 64, 'prev': 64},\n",
    "                    {'type': 'fc', 'n': self.out_dim, 'prev': 64}\n",
    "                ]\n",
    "\n",
    "            self.wb = []\n",
    "            initializer = tf.initializers.variance_scaling(scale=2.0)\n",
    "            #initializer = tf.initializers.variance_scaling()\n",
    "            #initializer = tf.contrib.layers.xavier_initializer()\n",
    "            #initializer = tf.random_normal_initializer()\n",
    "            for layer in self.layers:\n",
    "                if type(layer) is not list:\n",
    "                    if layer['type'] == 'conv':\n",
    "                        self.wb.append([tf.Variable(initializer([layer['height'], layer['width'], layer['in'], layer['out']])),\n",
    "                                        tf.Variable(tf.zeros([layer['out']]))])\n",
    "                    elif layer['type'] == 'fc':\n",
    "                        self.wb.append([tf.Variable(initializer([layer['prev'], layer['n']])),\n",
    "                                        tf.Variable(tf.zeros([layer['n']]))])\n",
    "                else:\n",
    "                    self.wb.append([])\n",
    "                    for l in layer:\n",
    "                        if l['type'] == 'conv':\n",
    "                            self.wb[-1].append([tf.Variable(initializer([l['height'], l['width'], l['in'], l['out']])),\n",
    "                                                tf.Variable(tf.zeros([l['out']]))])\n",
    "                        elif l['type'] == 'fc':\n",
    "                            self.wb[-1].append([tf.Variable(initializer([l['prev'], l['n']])),\n",
    "                                                tf.Variable(tf.zeros([l['n']]))])\n",
    "            self.forward = self.graph(self.X, self.M)\n",
    "        \n",
    "        \n",
    "    def graph(self, x, mask):\n",
    "        if self.atari:\n",
    "            x = x/255.0\n",
    "        xx = []\n",
    "        for i, wb in enumerate(self.wb):\n",
    "            \n",
    "            if type(wb[0]) is not list:\n",
    "                if self.layers[i]['type'] == 'conv':\n",
    "                    x = conv2d(x, wb[0], wb[1], strides=self.layers[i]['stride'])\n",
    "                else:\n",
    "                    x = tf.contrib.layers.flatten(x)\n",
    "                    x = tf.matmul(x, wb[0])+wb[1]\n",
    "                    if i+1<len(self.wb):\n",
    "                        x = tf.nn.relu(x)\n",
    "            else:\n",
    "                if not xx:\n",
    "                    x = tf.contrib.layers.flatten(x)\n",
    "                    xx = [tf.matmul(x, wb[0][0])+wb[0][1], tf.matmul(x, wb[1][0])+wb[1][1]]    \n",
    "                else:\n",
    "                    xx[0] = tf.matmul(xx[0], wb[0][0])+wb[0][1]\n",
    "                    xx[1] = tf.matmul(xx[1], wb[1][0])+wb[1][1]\n",
    "                if i+1<len(self.wb):\n",
    "                    xx[0] = tf.nn.relu(xx[0])\n",
    "                    xx[1] = tf.nn.relu(xx[1])\n",
    "        \n",
    "        if self.dueling:\n",
    "            x = xx[0] + tf.subtract(xx[1], tf.reduce_mean(xx[1], axis=1, keepdims=True))\n",
    "                \n",
    "        x = x*mask\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def copy_model_parameters(self, sess, copy_scope = 'main'):\n",
    "        \"\"\"\n",
    "        Copies the model parameters of one estimator to another.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session instance\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(copy_scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(self.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            update_ops.append(op)\n",
    "\n",
    "        sess.run(update_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvf9mgsKUvYW"
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, env, sess, lr=1e-4, gamma=0.99, buffer_size=100000,\n",
    "                 device=None, epoch_steps=5e4, evaluation_runs=5, batch_size=32, \n",
    "                 multiplier=1, target_network_update_freq = 2000, epsilon_decay_steps = 5e5,\n",
    "                 start_buffer=20000, atari=True, algo='dDQ', eval_rate=1):\n",
    "        \n",
    "        self.env = env\n",
    "        self.sess = sess\n",
    "        self.atari = atari\n",
    "        self.epoch = 0\n",
    "        self.losss = []\n",
    "        self.eval_rate = eval_rate\n",
    "        self.eval_rec = {}\n",
    "        self.algo = algo\n",
    "    \n",
    "        self.exp_buf = ExperienceBuffer(buffer_size)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.multiplier = multiplier\n",
    "        self.batch_size = batch_size*multiplier\n",
    "        \n",
    "        self.evaluation_runs = evaluation_runs\n",
    "        self.epoch_steps = epoch_steps\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.shape\n",
    "        self.epsilon_start = 1\n",
    "        self.epsilon_end = 0.1\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        \n",
    "        self.dueling = False\n",
    "        if 'd' in algo:\n",
    "            self.dueling = True\n",
    "\n",
    "        self.qnet = QNetwork(self.num_actions, self.obs_dim, atari=atari, scope='main', dueling=self.dueling)\n",
    "        \n",
    "        self.target_network = QNetwork(self.num_actions, self.obs_dim, atari=atari, scope='target', dueling=self.dueling)\n",
    "        \n",
    "        self.target_network_update_freq = target_network_update_freq\n",
    "        \n",
    "        self.qnet_loss = tf.reduce_mean(tf.losses.huber_loss(predictions = self.qnet.forward, labels=self.qnet.Y, delta=5.0))\n",
    "\n",
    "        self.qnet_optimizer = tf.train.AdamOptimizer(lr).minimize(self.qnet_loss)\n",
    "     \n",
    "        self.steps = 0\n",
    "        self.best_train_eval = -99999\n",
    "        self.best_test_eval = -99999\n",
    "\n",
    "        \n",
    "        self.initialize_buffer(start_buffer)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    def save_model(self, info='', folder='models'):\n",
    "        if not os.path.isdir('./'+folder):\n",
    "            os.mkdir('./'+folder)\n",
    "        if not os.path.isdir('./'+folder+'/'+self.env.unwrapped.spec.id):\n",
    "            os.mkdir('./'+folder+'/'+self.env.unwrapped.spec.id)\n",
    "            \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, './' + folder + '/' + self.env.unwrapped.spec.id + '/Epoch' + str(self.epoch) + str(info) + '.ckpt')\n",
    "        print('saved')\n",
    "        \n",
    "    def initialize_buffer(self, steps=20000):\n",
    "        done = True\n",
    "        obs = None\n",
    "        print(\"\\nInitializing buffer:\")\n",
    "        for _ in tqdm(range(steps)):\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "            init_obs = obs\n",
    "            act = self.env.action_space.sample()\n",
    "            obs, rew, done, _ = self.env.step(act)\n",
    "            self.exp_buf.add([init_obs, act, rew, obs, not done])\n",
    "\n",
    "            \n",
    "    def choose_action(self, obs):\n",
    "        if random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            nn_input = {self.qnet.X: [obs], self.qnet.M: np.array([self.num_actions*[1]])}\n",
    "            action = self.qnet.forward.eval(nn_input, session=self.sess)\n",
    "            action = np.argmax(action[0])\n",
    "            return action\n",
    "        \n",
    "\n",
    "    def train_epoch(self):\n",
    "        \n",
    "        self.epoch += 1\n",
    "        i = 0\n",
    "        print(\"====Epoch:\", self.epoch, \"====\")\n",
    "        print('steps', self.steps)\n",
    "        print('epsilon', self.epsilon)\n",
    "        \n",
    "        epoch_losss = []\n",
    "\n",
    "        rew_list = []\n",
    "        diff_list = []\n",
    "        with tqdm(total=self.epoch_steps) as pbar:\n",
    "\n",
    "            while i < self.epoch_steps:\n",
    "                \n",
    "                obs = self.env.reset()\n",
    "\n",
    "                done = False\n",
    "                tot_rew = 0\n",
    "                \n",
    "                while (not done) and (i < self.epoch_steps):\n",
    "                    \n",
    "                    step_num = i + (self.epoch-1)*self.epoch_steps\n",
    "                    \n",
    "                    for e in range(self.multiplier):\n",
    "                        init_obs = obs\n",
    "                        act = self.choose_action(obs)\n",
    "                        obs, rew, done, _ = self.env.step(act)\n",
    "\n",
    "                        tot_rew += rew\n",
    "                        self.exp_buf.add([init_obs, act, rew, obs, not done])\n",
    "\n",
    "                        if done:\n",
    "                            rew_list.append(tot_rew)\n",
    "                            break\n",
    "\n",
    "                    if not (self.steps)%self.target_network_update_freq:\n",
    "                        self.target_network.copy_model_parameters(self.sess, self.qnet.scope)\n",
    "                          \n",
    "                    # training \n",
    "\n",
    "                    sample = self.exp_buf.sample(self.batch_size)\n",
    "                    \n",
    "                    #assert all(s.shape == (84, 84, 4) for s in sample[:,3])\n",
    "                    DDQN = False\n",
    "                    if 'D' in self.algo:\n",
    "                        DDQN = True\n",
    "                    \n",
    "                    verb = False\n",
    "\n",
    "                    if DDQN:\n",
    "                        nn_input = {self.qnet.X: np.stack(sample[:,3]), self.qnet.M: np.ones((len(sample), self.num_actions))}\n",
    "                        nextqs = self.qnet.forward.eval(nn_input, session=self.sess)\n",
    "                        maxq_index = np.argmax(nextqs, axis=1)   \n",
    "                    \n",
    "                    nn_input = {self.target_network.X: np.stack(sample[:,3]), self.target_network.M: np.ones((len(sample), self.num_actions))}\n",
    "                    nextqs = self.target_network.forward.eval(nn_input, session=self.sess)\n",
    "                      \n",
    "                    if DDQN:\n",
    "                        target_q = nextqs[range(len(nextqs)), maxq_index]              \n",
    "                    else:\n",
    "                        target_q = np.amax(nextqs, 1)\n",
    "                    \n",
    "\n",
    "                    ys = sample[:,2] + sample[:,4] * self.gamma * target_q\n",
    "                    \n",
    "                    action_mask = np.eye(self.num_actions)[sample[:,1].astype(int)]\n",
    "\n",
    "                    ys = np.multiply(ys, action_mask.T).T\n",
    "                    \n",
    "                    _, loss = self.sess.run([self.qnet_optimizer, self.qnet_loss], feed_dict = {self.qnet.X: np.stack(sample[:,0]), self.qnet.M: action_mask, self.qnet.Y: ys})\n",
    "\n",
    "                    epoch_losss.append(loss)\n",
    "                    \n",
    "                    self.steps += 1\n",
    "                    i += 1\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        mean_acc = statistics.mean(rew_list)\n",
    "        print(\"\\nAvg rew:\", mean_acc)\n",
    "        if self.best_train_eval < mean_acc:\n",
    "            self.best_train_eval = mean_acc\n",
    "        print(\"losss:\", statistics.mean(list(map(float, epoch_losss))))\n",
    "        self.losss += epoch_losss\n",
    "        if not self.epoch%self.eval_rate:\n",
    "            results = self.run_evaluation()\n",
    "            self.eval_rec[self.epoch] = results\n",
    "            mean_acc = statistics.mean(results)\n",
    "            if self.best_test_eval < mean_acc:\n",
    "                self.best_test_eval = mean_acc\n",
    "                info = 'acc:' + str(mean_acc)\n",
    "                self.save_model(info)\n",
    "        \n",
    "\n",
    "    \n",
    "    def run_evaluation(self, evaluation_runs=5):\n",
    "        done = False\n",
    "        obs = None\n",
    "        \n",
    "        rs = []\n",
    "        for r in tqdm(range(evaluation_runs)):\n",
    "            tot_rew = 0\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            noopAct = random.randint(0,30)\n",
    "            for i in range(100000):\n",
    "                if done:\n",
    "                    break      \n",
    "                if i < noopAct: # https://arxiv.org/pdf/1511.06581.pdf\n",
    "                    act = 0\n",
    "                else:\n",
    "                        nn_input = {self.qnet.X: [obs], self.qnet.M: np.ones((1, self.num_actions))}\n",
    "                        action = self.qnet.forward.eval(nn_input, session=self.sess)\n",
    "                        act = np.argmax(action[0])\n",
    "                obs, rew, done, _ = self.env.step(act)\n",
    "                tot_rew += rew\n",
    "            rs.append(tot_rew)\n",
    "        print(\"test rewards:\", rs)\n",
    "        return rs\n",
    "        \n",
    "        \n",
    "    def display_agent(self):\n",
    "        import io\n",
    "        import base64\n",
    "        from IPython.display import HTML\n",
    "        uid = self.env.unwrapped.spec.id + '-' + 'Epoch' + str(self.epoch)\n",
    "        env = gym.wrappers.Monitor(self.env, \"./gym-results\", force=True, uid=uid)\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        noopAct = random.randint(0,30)\n",
    "        for i in range(50000):\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            if i < noopAct: # https://arxiv.org/pdf/1511.06581.pdf\n",
    "                    act = 0 # env.action_space.sample()\n",
    "            else:\n",
    "                nn_input = {self.qnet.X: [obs], self.qnet.M: np.ones((1, self.num_actions))}\n",
    "                action = self.qnet.forward.eval(nn_input, session=self.sess)\n",
    "                act = np.argmax(action[0])\n",
    "\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        return env   \n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        eps = self.epsilon_start - (self.epsilon_start-self.epsilon_end) * self.steps / self.epsilon_decay_steps\n",
    "        eps = max(self.epsilon_end, eps)\n",
    "        return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LNqUxigDRiCv",
    "outputId": "d449895a-d001-4734-8882-ee5d7324038f"
   },
   "outputs": [],
   "source": [
    "#env = gym.make('CartPole-v1')\n",
    "#env = gym.make('Pong-v4')\n",
    "#env = gym.make('PongNoFrameskip-v4')\n",
    "#env = gym.make('PongDeterministic-v4')\n",
    "#env = gym.make('Breakout-v0')\n",
    "\n",
    "tests = [ # algo: d=dueling, D=double\n",
    "        #{'env': 'CartPole-v1', 'algo': 'DQ', 'epochs': 200},\n",
    "        {'env': 'PongDeterministic-v4', 'algo': 'dDQ', 'epochs': 200},\n",
    "        #{'env': 'BreakoutDeterministic-v4', 'algo': 'dDQ', 'epochs': 200},\n",
    "\n",
    "        #{'env': 'Pong-v4', 'algo': 'dDQ', 'epochs': 200}, \n",
    "         #{'env': 'BreakoutDeterministic-v4', 'algo': 'dDQ', 'epochs': 200},\n",
    "         #{'env': 'PongDeterministic-v4', 'algo': 'dDQ', 'epochs': 200},\n",
    "        ]\n",
    "\n",
    "for test in tests:\n",
    "    env = gym.make(test['env'])\n",
    "    \n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    #sess = tf.Session()\n",
    "\n",
    "    atari = False\n",
    "    if 'atari' in str(env.env).lower():\n",
    "        print('atari')\n",
    "        env = wrap_deepmind(env, frame_stack=True, episode_life=False)\n",
    "        atari = True   \n",
    "        dqn_args = {\n",
    "            \"env\": env,\n",
    "            \"sess\": sess,\n",
    "            \"multiplier\": 1,\n",
    "            'lr':1e-4,\n",
    "            'buffer_size':100000,\n",
    "            'epoch_steps':5e4,\n",
    "            'gamma': 0.99,\n",
    "            'target_network_update_freq': 8000,\n",
    "            'epsilon_decay_steps': 6e5,\n",
    "            'start_buffer': 20000,\n",
    "            'batch_size': 32,\n",
    "            'atari': atari,\n",
    "\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        dqn_args = {\n",
    "        'env': env,\n",
    "        'sess': sess,\n",
    "        'lr': 1e-3,\n",
    "        'buffer_size':50000,\n",
    "        'epoch_steps':1e4,\n",
    "        'gamma': 0.95,\n",
    "        'target_network_update_freq': 1000,\n",
    "        'epsilon_decay_steps': 5e4,\n",
    "        'atari': atari,\n",
    "\n",
    "        }\n",
    "    dqn_args['algo'] = test['algo']\n",
    "\n",
    "    epochs = test['epochs']\n",
    "\n",
    "    dqn = DQN(**dqn_args)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(e)\n",
    "        dqn.train_epoch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-GiWL8lUz4W"
   },
   "outputs": [],
   "source": [
    "print(dqn.best_train_eval)\n",
    "\n",
    "dqn.run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eW9Igq9yfkAo"
   },
   "outputs": [],
   "source": [
    "env = dqn.display_agent()\n",
    "    #!pip uninstall tensorflow-gpu\n",
    "  #!pip install tensorflow-gpu\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "            <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    "        .format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dueling DQN_tensor 21.08.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
